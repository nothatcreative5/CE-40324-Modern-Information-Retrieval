{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGrfhZaqZEXw"
      },
      "source": [
        "<div style=\"direction:rtl;line-height:300%;\">\n",
        "<font face=\"XB Zar\" size=5>\n",
        "<div align=center>\n",
        "<font face=\"B Titr\" size=5>\n",
        "<p></p><p></p>\n",
        "بسمه تعالی\n",
        "<p></p>\n",
        "</font>\n",
        "<p></p>\n",
        "<font>\n",
        "<br>\n",
        "درس بازیابی پیشرفته اطلاعات\n",
        "<br>\n",
        "مدرس: دکتر سلیمانی\n",
        "</font>\n",
        "<p></p>\n",
        "<br>\n",
        "<font>\n",
        "<b>فاز سوم پروژه</b>\n",
        "</font>\n",
        "<br>\n",
        "<br>\n",
        "موعد تحویل:  ساعت ۶ صبح ۸ تیر<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<font>\n",
        "دانشگاه صنعتی شریف\n",
        "<br>\n",
        "دانشکده مهندسی کامپیوتر\n",
        "<br>\n",
        "<br>\n",
        "</font>\n",
        "</div>\n",
        "</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yHe80ULZEX1"
      },
      "source": [
        "<div dir=\"rtl\">\n",
        "<font face=\"XB Zar\" size=5>\n",
        "    <h1>\n",
        "    <b>مقدمه</b>\n",
        "    </h1>\n",
        "    <p></p>\n",
        "    <p></p>\n",
        "</font>\n",
        "<font face=\"XB Zar\" size=3>\n",
        "     <br>\n",
        "    در این فاز از پروژه، تمرکز ما بر\n",
        "    crawling\n",
        "    و تحلیل مقالات استخراج‌شده از اینترنت خواهد بود. ما با بررسی تکنیک های مختلف  \n",
        "    web crawling\n",
        "    برای استخراج مقالات و سایر اطلاعات مرتبط از وب شروع خواهیم کرد.\n",
        "    <br>\n",
        "    در مرحله بعد، الگوریتم های تجزیه و تحلیل  لینک مانند\n",
        "    PageRank\n",
        "    و\n",
        "    HITS\n",
        "    را برای تعیین اهمیت این مقالات بر اساس نقل قول‌ها، ارجاعات یا اشکال دیگر پیوندها اعمال خواهیم‌کرد. ما همچنین یاد خواهیم‌گرفت که چگونه یک الگوریتم\n",
        "    PageRank\n",
        "    شخصی‌سازی‌شده را پیاده‌سازی کنیم که ترجیحات کاربر را برای ارائه نتایج مرتبط تر در نظر می‌گیرد.\n",
        "    <br>\n",
        "    در بخش سوم این مرحله، یک موتور جستجوی شخصی‌سازی شده را پیاده‌سازی میکنیم و یاد می‌گیریم که چگونه موتور جستجویی بسازیم که نتایجی را بر اساس ترجیحات کاربر ارائه دهد.\n",
        "    <br>\n",
        "در نهایت، ما یک\n",
        "    task\n",
        "     در مورد\n",
        "    recommendation system\n",
        "    ها خواهیم‌داشت، که در آن از تکنیک های مختلف برای توصیه مقالات یا صفحات وب به کاربران بر اساس ترجیحات و رفتار آنها استفاده خواهیم کرد.\n",
        "    <br>\n",
        "     تنها زبان قابل قبول برای پروژه پایتون است. محدودیت استفاده از کتاب‌خانه‌های آماده در هر بخش مشخص شده است. در انتهای پروژه قرار است یک سیستم یکپارچه‌ی جست‌و‌جو داشته باشید، بنابراین به پیاده‌سازی هر چه بهتر این فاز توجه داشته باشید.\n",
        "</font>\n",
        "</div>\n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "_jL2RJzXZEX3"
      },
      "source": [
        "<div dir=\"rtl\">\n",
        "<font face=\"XB Zar\" size=4>\n",
        "    <h1>\n",
        "    <b>پیاده‌سازی Crawler (۴۰ نمره)</b>\n",
        "    </h1>\n",
        "</font>\n",
        "    <br>\n",
        "\n",
        "<font face=\"XB Zar\" size=3>\n",
        "   در این بخش باید یک Crawler\n",
        "    برای واکشی اطلاعات تعدادی مقاله از سایت <a href=\"https://www.semanticscholar.org/\">Semantic Scholar</a> پیاده سازی کنید.\n",
        "   اطلاعات واکشی شده باید حاوی موارد زیر باشد.\n",
        "</font>\n",
        "</div>\n",
        "<br>\n",
        "<table dir=\"ltr\" style=\"width: 100%; border-collapse: collapse;\">\n",
        "  <tr>\n",
        "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">ID</th>\n",
        "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Title</th>\n",
        "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Abstract</th>\n",
        "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Publication Year</th>\n",
        "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Authors</th>\n",
        "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Related Topics</th>\n",
        "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Citation Count</th>\n",
        "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Reference Count</th>\n",
        "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">References</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Unique ID of the paper</td>\n",
        "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Title of the paper</td>\n",
        "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Abstract of the paper</td>\n",
        "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Publication year</td>\n",
        "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Name of the first author, ..., Name of the last author</td>\n",
        "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">topic1, topic2, ...</td>\n",
        "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">number of citations of the paper</td>\n",
        "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">number of references of the paper</td>\n",
        "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">ID of the first reference, ..., ID of the tenth reference</td>\n",
        "  </tr>\n",
        "</table>\n",
        "    <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "yw5MWLxjZEX4"
      },
      "source": [
        "<div dir=\"rtl\">\n",
        "\n",
        "<font face=\"XB Zar\" size=3>\n",
        "  ابتدا فرایند واکشی را از ۵ مقاله‌ی هر استاد شروع کنید و\n",
        "    ۱۰\n",
        "    مرجع اول هر مقاله را به صف مقالات اضافه کنید.\n",
        "    فرایند واکشی را نا جایی ادامه دهید که اطلاعات ۲۰۰۰ مقاله را داشته باشید.\n",
        "    اطلاعات مقالات را در فایل crawled_paper_profName.json ذخیره کنید.\n",
        "</font>\n",
        "</div>\n",
        "\n",
        "<div dir=\"rtl\">\n",
        "\n",
        "<font face=\"XB Zar\" size=3>\n",
        "  در پیاده سازی Crawler به موارد زیر دقت کنید.\n",
        "    \n",
        "    \n",
        "<ul>\n",
        "<li>حق استفاده از api سایت semantic scholar را ندارید.</li>\n",
        "<li>برای واکشی می‌توانید از پکیج‌هایی مثل <a href=\"https://www.selenium.dev/selenium/docs/api/py/\">Selenium</a> و یا <a href=\"https://github.com/scrapy/scrapy\">Scrapy</a>  استفاده کنید. استفاده از پکیج‌های دیگر نیز مجاز است. همچنین برای پارس اطلاعات واکشی شده می‌توانید از پکیج <a href=\"https://pypi.org/project/beautifulsoup4/\">Beautiful Soup</a> استفاده کنید.\n",
        "</li>\n",
        "<li>بین هر بار درخواست از سایت یک فاصله چند ثانیه‌ای بدهید.</li>\n",
        "<li>در زمان تحویل کد Crawler شما اجرا خواهد شد و صحت آن بررسی خواهد شد.</li>\n",
        "<li>در صورتی که ‌Crawler شما به دچار اروری مثل request timeout شد نباید کار خود را متوقف کند.</li>\n",
        "</ul>\n",
        "\n",
        "\n",
        "</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmtopesBZG6_",
        "outputId": "38f11fd9-d15b-43fd-d70d-4a6b5a37fd23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dyHBvM4pv50C"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def getID(url):\n",
        "    pattern = r\"/([^/]+)$\"\n",
        "\n",
        "    # Extract the ID using regex\n",
        "    match = re.search(pattern, url)\n",
        "\n",
        "    if match:\n",
        "        id = match.group(1)\n",
        "        return id\n",
        "    else:\n",
        "        print(\"ID not found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "x9I19qZ39bPg"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "\n",
        "class SemanticFetcher:\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        self.API_KEY = \"37adfa1c38bc1cf882587ac3131c61b8\"\n",
        "        self.SCRAPPER_API = 'http://api.scraperapi.com'\n",
        "\n",
        "\n",
        "    def fetch_paper(self, url):\n",
        "        payload = {'api_key': self.API_KEY, 'url': url}\n",
        "        r = requests.get(self.SCRAPPER_API, params=payload)\n",
        "\n",
        "        if r.status_code != 200:\n",
        "            print(f'fetching {url} failed with status {r.status_code}')\n",
        "            return\n",
        "\n",
        "        soup = BeautifulSoup(r.text, 'html.parser')\n",
        "\n",
        "        return soup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1fi6BoyIa12p"
      },
      "outputs": [],
      "source": [
        "class PaperParser:\n",
        "\n",
        "\n",
        "    def __init__(self):\n",
        "        self.fetcher = SemanticFetcher()\n",
        "\n",
        "    def parse_paper(self, paper):\n",
        "\n",
        "        soup = self.fetcher.fetch_paper(paper[\"URL\"])\n",
        "\n",
        "        paper[\"ID\"] = getID(paper[\"URL\"])\n",
        "        del paper[\"URL\"]\n",
        "\n",
        "        pre_text = soup.find('pre').text\n",
        "\n",
        "        year_match = re.search(r'year={(\\d+)}', pre_text)\n",
        "        author_match = re.search(r'author={(.+?)}', pre_text)\n",
        "\n",
        "        year = '' if not year_match else year_match.group(1)\n",
        "        authors = [] if not author_match else \\\n",
        "         [author.strip() for author in author_match.group(1).split(' and ')]\n",
        "\n",
        "\n",
        "        paper['Related Topics'] = self.get_related_topics(soup)\n",
        "        paper['Citation Count'], paper['Reference Count'] = self.get_counts(soup)\n",
        "        paper['Title'], paper['Abstract'] = self.get_title_abstract(soup)\n",
        "        paper['Publication Year'] = year\n",
        "        paper['Authors'] = authors\n",
        "\n",
        "        children, paper = self.get_references(soup, paper,paper['Citation Count'])\n",
        "\n",
        "        return children, paper\n",
        "\n",
        "    def get_title_abstract(self, soup):\n",
        "        h1_element = soup.find('h1', attrs={'data-test-id': 'paper-detail-title'})\n",
        "        title = h1_element.text\n",
        "\n",
        "        span_element = soup.find('span', attrs={'data-test-id': 'text-truncator-text'})\n",
        "\n",
        "        abstract = soup.findAll('script', {\n",
        "                'type': 'application/ld+json',\n",
        "                'class': 'schema-data',\n",
        "        })[0]\n",
        "\n",
        "        abstract = json.loads(abstract.text)\n",
        "        abstract = abstract['@graph'][1][0]['abstract']\n",
        "\n",
        "        return title, abstract\n",
        "\n",
        "\n",
        "    def get_references(self, soup, paper, citation):\n",
        "\n",
        "        PREFIX = \"https://www.semanticscholar.org\"\n",
        "\n",
        "        children = []\n",
        "        paper['References'] = []\n",
        "\n",
        "        links = soup.find_all('a', class_='link-button--show-visited')\n",
        "\n",
        "        if links is None:\n",
        "            return children, paper\n",
        "\n",
        "        links = links[min(citation, 10):]\n",
        "\n",
        "        for link in links:\n",
        "            link = PREFIX + link.get('href')\n",
        "            paper['References'].append(getID(link))\n",
        "            child = {}\n",
        "            child[\"URL\"] = link\n",
        "            children.append(child)\n",
        "\n",
        "        return children, paper\n",
        "\n",
        "    def get_related_topics(self, soup):\n",
        "\n",
        "        related_topics_html = soup.findAll('li', class_='paper-meta-item', recursive=True)\n",
        "        related_topics = ''\n",
        "\n",
        "        for topic in related_topics_html:\n",
        "            if topic.find('span') is not None:\n",
        "                continue\n",
        "            else:\n",
        "                related_topics = topic.text.split(', ')\n",
        "\n",
        "        return related_topics\n",
        "\n",
        "\n",
        "    def get_counts(self, soup):\n",
        "\n",
        "        h2_elements = soup.find_all('h2', class_='dropdown-filters__result-count__header dropdown-filters__result-count__citations')\n",
        "\n",
        "        citation_count = 0\n",
        "        reference_count = 0\n",
        "\n",
        "        for h2_element in h2_elements:\n",
        "            content = h2_element.text\n",
        "            if \"References\" in content:\n",
        "                reference_count  = int(content.split()[0].replace(',', ''))\n",
        "            elif \"Citations\" in content:\n",
        "                citation_count = int(content.split()[0].replace(',', ''))\n",
        "\n",
        "        return citation_count, reference_count\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwGURiclZVo8",
        "outputId": "fef27bda-51b7-414c-fc81-07697ebc38e2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'URL': 'https://www.semanticscholar.org/paper/The-Eighth-Visual-Object-Tracking-VOT2020-Challenge-Kristan-Leonardis/12508951ba96b7d4c0906ed95542287d3ebdfd95'},\n",
              " {'URL': 'https://www.semanticscholar.org/paper/Benign-and-malignant-breast-tumors-classification-Rouhi-Jafari/c6db34ade32b3681a92068b22a354903b2953d52'},\n",
              " {'URL': 'https://www.semanticscholar.org/paper/Event-Detection-and-Summarization-in-Soccer-Videos-Tavassolipour-Karimian/ac2e54cec3aa2d1e67288d00c7fce7b7b17f9a73'},\n",
              " {'URL': 'https://www.semanticscholar.org/paper/An-efficient-PCA-based-color-transfer-method-Abadpour-Kasaei/53fc0415e0d00f9691994a49b8232a1cc2dfad5f'},\n",
              " {'URL': 'https://www.semanticscholar.org/paper/Deep-Learning-for-Visual-Tracking%3A-A-Comprehensive-Marvasti-Zadeh-Cheng/1fbb4201af091aef55360f113ba35814063923e4'}]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "profs = ['Kasaei', 'Rabiee', 'Sharifi', 'Soleymani', 'Rohban']\n",
        "PATH = \"/content/drive/MyDrive/MIR/Assignment 3\"\n",
        "urls = {}\n",
        "\n",
        "for prof in profs:\n",
        "    file_path = f\"{PATH}/{prof}.txt\"\n",
        "    urls[prof] = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.read().splitlines()\n",
        "        for line in lines:\n",
        "            paper = {}\n",
        "            paper[\"URL\"] = line\n",
        "            urls[prof].append(paper)\n",
        "\n",
        "urls['Kasaei']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIJIPT2O_YVu",
        "outputId": "d4ea286d-d97b-455f-da4e-8333de52e7c0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 400/400 [20:48<00:00,  3.12s/it]\n",
            "100%|██████████| 400/400 [22:07<00:00,  3.32s/it]\n",
            "100%|██████████| 400/400 [18:48<00:00,  2.82s/it]\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "parser = PaperParser()\n",
        "crawled = {}\n",
        "seen = set()\n",
        "\n",
        "for prof in ['Sharifi', 'Soleymani', 'Rohban']:\n",
        "    count = 0\n",
        "    crawled[prof] = []\n",
        "    for i in tqdm(range(400)):\n",
        "        paper = urls[prof][0]\n",
        "        urls[prof].pop(0)\n",
        "        if paper[\"URL\"].lower() in seen:\n",
        "            continue\n",
        "        seen.add(paper[\"URL\"].lower())\n",
        "        try:\n",
        "            children, paper = parser.parse_paper(paper)\n",
        "        except:\n",
        "            continue\n",
        "        crawled[prof].append(paper)\n",
        "        for child in children:\n",
        "            if child[\"URL\"].lower() in seen:\n",
        "                continue\n",
        "            else:\n",
        "                urls[prof].append(child)\n",
        "        if len(urls[prof]) == 0:\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R93lqVdIlFsm"
      },
      "outputs": [],
      "source": [
        "PATH = \"/content/drive/MyDrive/MIR/Assignment 3\"\n",
        "for prof in profs:\n",
        "    file_path = PATH +  f\"/crawled_paper_{prof}.json\"\n",
        "    with open(file_path, 'w') as file:\n",
        "        json.dump(crawled[prof], file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "profs = ['Kasaei', 'Rabiee', 'Sharifi', 'Soleymani', 'Rohban']\n",
        "\n",
        "crawled = {}\n",
        "\n",
        "for prof in profs:\n",
        "    file_path = f\"/content/drive/MyDrive/MIR/Assignment 3/crawled_paper_{prof}.json\"\n",
        "    with open(file_path, 'r') as file:\n",
        "        crawled[prof] = json.load(file)"
      ],
      "metadata": {
        "id": "KgsdkN33CIud"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "SfZlpPROZEX5"
      },
      "source": [
        "<div dir=\"rtl\">\n",
        "<font face=\"XB Zar\" size=4>\n",
        "    <h1>\n",
        "    <b>PageRank\n",
        "        شخصی‌سازی‌شده\n",
        "        (۲۰ نمره)</b>\n",
        "    </h1>\n",
        "</font>\n",
        "    <br>\n",
        "<font face=\"XB Zar\" size=3>\n",
        "در این بخش، الگوریتم\n",
        "    PageRank\n",
        "    شخصی‌سازی‌شده را پیاده‌سازی می‌کنیم که توسعه‌ای از الگوریتم\n",
        "    PageRank\n",
        "    است که ترجیحات کاربر را در نظر می‌گیرد. الگوریتم\n",
        "    PageRank\n",
        "    شخصی‌سازی‌شده گره‌ها را در یک گراف بر اساس اهمیت آنها برای کاربر رتبه‌بندی می‌کند، نه بر اساس اهمیت کلی آنها در نمودار.\n",
        "    \n",
        "</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ordered_set"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyk-OxC4ix37",
        "outputId": "554370db-9bf0-48e9-9de1-8311b0e81428"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ordered_set\n",
            "  Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: ordered_set\n",
            "Successfully installed ordered_set-4.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "djEyAuzaNhDv"
      },
      "outputs": [],
      "source": [
        "from ordered_set import OrderedSet\n",
        "\n",
        "\n",
        "def create_adjacency_matrix(graph):\n",
        "    nodes = OrderedSet(graph.keys()).union(*graph.values())\n",
        "\n",
        "    adjacency_matrix = {node: [0] * len(nodes) for node in nodes}\n",
        "\n",
        "    for node, outgoing_edges in graph.items():\n",
        "        for edge in outgoing_edges:\n",
        "            adjacency_matrix[node][list(nodes).index(edge)] = 1\n",
        "\n",
        "    matrix = np.array(list(adjacency_matrix.values()))\n",
        "\n",
        "    row_sums = matrix.sum(axis=1)\n",
        "    N = len(nodes)\n",
        "    zero_rows = np.where(row_sums == 0)[0]\n",
        "    matrix[zero_rows] = 1\n",
        "\n",
        "    row_sums = matrix.sum(axis=1)\n",
        "\n",
        "    matrix = matrix / (row_sums[:, np.newaxis])\n",
        "\n",
        "    alpha = 0.1\n",
        "\n",
        "    matrix =  (1 - alpha) * matrix + alpha * np.ones(matrix.shape) / N\n",
        "\n",
        "    return matrix, nodes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "zvNb3e-0ZEX6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from typing import Dict, List\n",
        "\n",
        "def pagerank(graph: Dict[str, List[str]]) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Returns the personalized PageRank scores for the nodes in the graph, given the user's preferences.\n",
        "\n",
        "    Parameters:\n",
        "    graph (Dict[str, List[str]]): The graph represented as a dictionary of node IDs and their outgoing edges.\n",
        "\n",
        "    Returns:\n",
        "    Dict[str, float]: A dictionary of node IDs and their personalized PageRank scores.\n",
        "    \"\"\"\n",
        "    matrix, nodes = create_adjacency_matrix(graph)\n",
        "\n",
        "    eigenvalues, eigenvectors = np.linalg.eig(matrix.T)\n",
        "\n",
        "    if np.isclose(eigenvalues, 1).any():\n",
        "        index_of_1 = np.where(np.isclose(eigenvalues, 1))[0][0]\n",
        "    else:\n",
        "        index_of_1 = np.argmax(eigenvalues)\n",
        "\n",
        "    pagerank_vector = eigenvectors[:, index_of_1]\n",
        "    pagerank_vector /= np.sum(pagerank_vector)\n",
        "\n",
        "    pagerank_scores = {node: score for node, score in zip(nodes, pagerank_vector)}\n",
        "\n",
        "    return pagerank_scores\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "hkcPWq5LZEX8"
      },
      "source": [
        "<div dir=\"rtl\">\n",
        "<font face=\"XB Zar\" size=3>\n",
        "در این بخش از الگوریتم\n",
        "PageRank\n",
        "شخصی‌سازی‌شده که در قسمت قبلی پیاده‌سازی شده‌است برای\n",
        "شناسایی مقالات مهم مرتبط با حوزه‌ی کاری یک استاد\n",
        "خاص استفاده می‌کنیم. این تابع، یک\n",
        "    field\n",
        "    را به عنوان ورودی دریافت می‌کند. خروجی نیز\n",
        "مقالات برتری که بیشترین ارتباط را با آن زمینه دارند؛ خواهدبود.\n",
        "</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9djr-mmpZEX8"
      },
      "outputs": [],
      "source": [
        "def important_articles(Professor: str, count = 10) -> List[str]:\n",
        "    \"\"\"\n",
        "    Returns the most important articles in the field of given professor, based on the personalized PageRank scores.\n",
        "\n",
        "    Parameters:\n",
        "    Professor (str): Professor's name.\n",
        "\n",
        "    Returns:\n",
        "    List[str]: A list of article IDs representing the most important articles in the field of given professor.\n",
        "    \"\"\"\n",
        "\n",
        "    file_path = f\"/content/drive/MyDrive/MIR/Assignment 3/crawled_paper_{Professor}.json\"\n",
        "\n",
        "    with open(file_path, 'r') as file:\n",
        "        articles = json.load(file)\n",
        "\n",
        "    graph = {}\n",
        "\n",
        "    for article in articles:\n",
        "        if article[\"ID\"] in graph:\n",
        "            continue\n",
        "        else:\n",
        "            graph[article[\"ID\"]] = []\n",
        "            for reference in article[\"References\"]:\n",
        "                graph[article[\"ID\"]].append(reference)\n",
        "\n",
        "    scores = pagerank(graph)\n",
        "    return sorted(scores.items(), key=lambda x: x[1], reverse=True)[:count]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rd8r_7ThZDTS",
        "outputId": "7536c320-9c51-46ef-d6c3-b331290de885"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('5a7571db7df03cca52c48f89595c4abefeb51e5c', (0.0017503298755893353+0j)),\n",
              " ('89b52ef051679634365a0f646e18b59537b396d6', (0.0015436805524707477+0j)),\n",
              " ('4b1a47709d0546e5bc614bf9a521c550e6881d04', (0.0015406651789352276+0j)),\n",
              " ('564649846003db680733697947f974a1ef03c4ea', (0.001506610491655106+0j)),\n",
              " ('505f48d8236eb25f871da272c2ac2fe4b41ea289', (0.0014978890163668422+0j)),\n",
              " ('eda3368a5198ca55768b07b6f5667aea28baf2cd', (0.0014802671025683602+0j)),\n",
              " ('d6151de801659937574c3efe13c2d207e9e2f2cd', (0.0013011660656085744+0j)),\n",
              " ('201631fbc3f7d7cb2c1ddfaf82278cad5e44f2f9', (0.0012713983127243074+0j)),\n",
              " ('b7d540cd0de72e984cdec44afa4a4d039cfd5eea', (0.001268885490644621+0j)),\n",
              " ('5138a7d65d6875191572291f3458b4de149b4d2a', (0.001187697992029479+0j)),\n",
              " ('edea2f25d705d43ce90f725eed62f7dba6fbd50f', (0.0011848403937732217+0j)),\n",
              " ('34c44883a6152c5298f2c452670c1127072400e6', (0.0010977500396703948+0j)),\n",
              " ('bfba194dfd9c7c27683082aa8331adc4c5963a0d', (0.001091221440048318+0j)),\n",
              " ('9926020dda21874dc7a5ef1511bae6c4cef5ecb9', (0.001086606813570409+0j)),\n",
              " ('f0bafbf9cf2dfb1fd7e439e2336f1dd3af19478c', (0.0010853533380148518+0j)),\n",
              " ('d3448a792e771d8cd49a4e0c3b08ae5b3d51e51f', (0.0010429969250255581+0j)),\n",
              " ('ecab8d10ca24b53eca2bf1580e8cd03fe7984676', (0.0010327729087700977+0j)),\n",
              " ('71f98dc5cc9409ceb35f057eb5cbe6ede187a1ba', (0.0010053678899042114+0j)),\n",
              " ('6767812e114c426d45ea83894b156f7906e525cd', (0.0009917389333682677+0j)),\n",
              " ('c63a34ac6a4e049118070e707ca7679fbb132d33', (0.000975895417651559+0j))]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "important_articles('Kasaei', count = 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "99Nu3Q7nZEX9"
      },
      "source": [
        "<div dir=\"rtl\">\n",
        "<font face=\"XB Zar\" size=4>\n",
        "    <h1>\n",
        "    <b>جستجو شخصی‌سازی‌شده (۱۰ نمره)</b>\n",
        "    </h1>\n",
        "</font>\n",
        "    <br>\n",
        "<font face=\"XB Zar\" size=3>\n",
        "الگوریتم جست‌و‌جویی که در فازهای گذشته پیاده‌سازی کرده‌اید را به گونه‌ای تغییر دهید که نتایج به دست آمده جست‌و‌جو بر حسب علایق فرد مرتب شوند. از قضیه‌ی خطی بودن برای این کار استفاده کنید.\n",
        "    \n",
        "</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3oatFqci3BE",
        "outputId": "301579c5-46ce-4370-976c-17bd7a3d1609"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0t002cjTiz13",
        "outputId": "b517f396-c77c-4a7f-c0cd-c7c63b14a22e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['an', 'abstract', 'is', 'a', 'summary', 'of', 'the', 'main', 'article']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "def clean_data(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [token for token in tokens if token not in string.punctuation]\n",
        "\n",
        "    stemmer = PorterStemmer()\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "\n",
        "clean_data(\"An abstract is a summary of the main article.\") # return [\"an\", \"abstract\", \"is\", \"a\", \"summary\", \"of\", \"the\", \"main\", \"article\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "CVirxIYUoJEm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def create_dataframe(crawled):\n",
        "    keys_to_include = ['ID', 'Title', 'Abstract']\n",
        "    df = pd.DataFrame([{k: d[k] for k in keys_to_include} for d in crawled])\n",
        "\n",
        "\n",
        "\n",
        "    # df = pd.read_csv('data.csv')\n",
        "    df['Abstract'] = df['Abstract'].fillna('')\n",
        "    avg_lens = [0, 0]\n",
        "\n",
        "    # iter rows\n",
        "    for index, row in df.iterrows():\n",
        "        title, abstract = clean_data(row[\"Title\"]), clean_data(row[\"Abstract\"])\n",
        "        avg_lens[0] += len(title)\n",
        "        avg_lens[1] += len(abstract)\n",
        "\n",
        "        row[\"Title\"] = title\n",
        "        row[\"Abstract\"] = abstract\n",
        "\n",
        "    avg_lens[0] /= len(df)\n",
        "    avg_lens[1] /= len(df)\n",
        "\n",
        "    return df, avg_lens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "HZQXckIyoeef"
      },
      "outputs": [],
      "source": [
        "datasets = {}\n",
        "\n",
        "for prof in profs:\n",
        "    file_path = f\"/content/drive/MyDrive/MIR/Assignment 3/crawled_paper_{prof}.json\"\n",
        "    with open(file_path, 'r') as file:\n",
        "        articles = json.load(file)\n",
        "    datasets[prof] = create_dataframe(articles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Fr_DirjbkGtL",
        "outputId": "76d8d46f-73d2-44cc-d101-08a957a5c8dd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                         ID  \\\n",
              "0  12508951ba96b7d4c0906ed95542287d3ebdfd95   \n",
              "1  c6db34ade32b3681a92068b22a354903b2953d52   \n",
              "2  ac2e54cec3aa2d1e67288d00c7fce7b7b17f9a73   \n",
              "3  53fc0415e0d00f9691994a49b8232a1cc2dfad5f   \n",
              "4  1fbb4201af091aef55360f113ba35814063923e4   \n",
              "\n",
              "                                               Title  \\\n",
              "0  [the, eighth, visual, object, tracking, vot202...   \n",
              "1  [benign, and, malignant, breast, tumor, classi...   \n",
              "2  [event, detection, and, summarization, in, soc...   \n",
              "3  [an, efficient, pca-based, color, transfer, me...   \n",
              "4  [deep, learning, for, visual, tracking, a, com...   \n",
              "\n",
              "                                            Abstract  \n",
              "0  [a, significant, novelty, is, introduction, of...  \n",
              "1  [semantic, scholar, extracted, view, of, ``, b...  \n",
              "2  [a, novel, bayesian, network-based, method, th...  \n",
              "3  [semantic, scholar, extracted, view, of, ``, a...  \n",
              "4  [this, survey, aim, to, systematically, invest...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-47c0f869-56a5-437e-b187-813d76d30c61\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Title</th>\n",
              "      <th>Abstract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12508951ba96b7d4c0906ed95542287d3ebdfd95</td>\n",
              "      <td>[the, eighth, visual, object, tracking, vot202...</td>\n",
              "      <td>[a, significant, novelty, is, introduction, of...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>c6db34ade32b3681a92068b22a354903b2953d52</td>\n",
              "      <td>[benign, and, malignant, breast, tumor, classi...</td>\n",
              "      <td>[semantic, scholar, extracted, view, of, ``, b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ac2e54cec3aa2d1e67288d00c7fce7b7b17f9a73</td>\n",
              "      <td>[event, detection, and, summarization, in, soc...</td>\n",
              "      <td>[a, novel, bayesian, network-based, method, th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>53fc0415e0d00f9691994a49b8232a1cc2dfad5f</td>\n",
              "      <td>[an, efficient, pca-based, color, transfer, me...</td>\n",
              "      <td>[semantic, scholar, extracted, view, of, ``, a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1fbb4201af091aef55360f113ba35814063923e4</td>\n",
              "      <td>[deep, learning, for, visual, tracking, a, com...</td>\n",
              "      <td>[this, survey, aim, to, systematically, invest...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-47c0f869-56a5-437e-b187-813d76d30c61')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-47c0f869-56a5-437e-b187-813d76d30c61 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-47c0f869-56a5-437e-b187-813d76d30c61');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "datasets['Kasaei'][0].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "xcbD6bdUqTJb",
        "outputId": "42c161ed-d681-444e-f69e-201c079be39e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                         ID  \\\n",
              "0  5ca94050fcf3382b50ec44629c0dda80c8843558   \n",
              "1  6885c45614f78f9d2e7cc8ef11b3c38b34e67f7d   \n",
              "2  6f0685d61328f0f90972fe822258d574b74e9c7a   \n",
              "3  c626a9d75dfd73e26cf30793d5ef71527cd9fa95   \n",
              "4  e1dcd7fd049ae2ae3b93295d8ea360cafc00f9da   \n",
              "\n",
              "                                               Title  \\\n",
              "0  [spatial-aware, dictionary, learning, for, hyp...   \n",
              "1  [multiresolution, knowledge, distillation, for...   \n",
              "2  [a, hybrid, deep, learning, architecture, for,...   \n",
              "3  [novel, dataset, for, fine-grained, abnormal, ...   \n",
              "4                [deep, private-feature, extraction]   \n",
              "\n",
              "                                            Abstract  \n",
              "0  [a, structured, dictionary-based, model, for, ...  \n",
              "1  [this, work, proposes, to, use, the, ``, disti...  \n",
              "2  [this, article, present, a, hybrid, approach, ...  \n",
              "3  [this, work, present, a, novel, crowd, dataset...  \n",
              "4  [the, log-rank, privacy, is, introduced, and, ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2464ff9e-d4e8-41d4-b592-19ae24f6d9fd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Title</th>\n",
              "      <th>Abstract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5ca94050fcf3382b50ec44629c0dda80c8843558</td>\n",
              "      <td>[spatial-aware, dictionary, learning, for, hyp...</td>\n",
              "      <td>[a, structured, dictionary-based, model, for, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6885c45614f78f9d2e7cc8ef11b3c38b34e67f7d</td>\n",
              "      <td>[multiresolution, knowledge, distillation, for...</td>\n",
              "      <td>[this, work, proposes, to, use, the, ``, disti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6f0685d61328f0f90972fe822258d574b74e9c7a</td>\n",
              "      <td>[a, hybrid, deep, learning, architecture, for,...</td>\n",
              "      <td>[this, article, present, a, hybrid, approach, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>c626a9d75dfd73e26cf30793d5ef71527cd9fa95</td>\n",
              "      <td>[novel, dataset, for, fine-grained, abnormal, ...</td>\n",
              "      <td>[this, work, present, a, novel, crowd, dataset...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>e1dcd7fd049ae2ae3b93295d8ea360cafc00f9da</td>\n",
              "      <td>[deep, private-feature, extraction]</td>\n",
              "      <td>[the, log-rank, privacy, is, introduced, and, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2464ff9e-d4e8-41d4-b592-19ae24f6d9fd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2464ff9e-d4e8-41d4-b592-19ae24f6d9fd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2464ff9e-d4e8-41d4-b592-19ae24f6d9fd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "datasets['Rabiee'][0].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "4AZeUBarlbbW"
      },
      "outputs": [],
      "source": [
        "def construct_positional_indexes(df):\n",
        "\n",
        "    positional_index = {}\n",
        "\n",
        "\n",
        "    '''\n",
        "        Save the length of each part of the document in the positional index\n",
        "    '''\n",
        "\n",
        "    for i, row in df.iterrows():\n",
        "        paper_id = i + 1\n",
        "        title = row['Title']\n",
        "        abstract = row['Abstract']\n",
        "\n",
        "\n",
        "        '''\n",
        "            0 -> title\n",
        "            1 -> abstract\n",
        "\n",
        "            token = [\n",
        "            title -> [paper_id, length, [positions]],\n",
        "            abstract -> [paper_id, length, [positions]]]\n",
        "\n",
        "        '''\n",
        "\n",
        "        # Create positional index for title tokens\n",
        "        for i, token in enumerate(title):\n",
        "            if token not in positional_index:\n",
        "                positional_index[token] = []\n",
        "            if len(positional_index[token]) == 0 or positional_index[token][-1][0] != paper_id:\n",
        "                positional_index[token].append([paper_id,[len(title)],[len(abstract)]])\n",
        "            title_list = positional_index[token][-1][1]\n",
        "            title_list.insert(-1, i)\n",
        "\n",
        "\n",
        "        for i, token in enumerate(abstract):\n",
        "            if token not in positional_index:\n",
        "                positional_index[token] = []\n",
        "            if len(positional_index[token]) == 0 or positional_index[token][-1][0] != paper_id:\n",
        "                positional_index[token].append([paper_id,[len(title)],[len(abstract)]])\n",
        "            abstract_list = positional_index[token][-1][2]\n",
        "            abstract_list.insert(-1, i)\n",
        "\n",
        "    return positional_index\n",
        "\n",
        "docs = {}\n",
        "for prof in profs:\n",
        "    docs[prof] = construct_positional_indexes(datasets[prof][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "1p8RV8dom08M"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "class bcolors:\n",
        "    HEADER = '\\033[95m'\n",
        "    OKBLUE = '\\033[94m'\n",
        "    OKCYAN = '\\033[96m'\n",
        "    OKGREEN = '\\033[92m'\n",
        "    WARNING = '\\033[93m'\n",
        "    FAIL = '\\033[91m'\n",
        "    ENDC = '\\033[0m'\n",
        "    BOLD = '\\033[1m'\n",
        "    UNDERLINE = '\\033[4m'\n",
        "\n",
        "\n",
        "def print_single(text, query, firstk = 20):\n",
        "\n",
        "    for word in text[:firstk]:\n",
        "        if clean_data(word)[0] in query:\n",
        "            print(bcolors.OKGREEN + bcolors.BOLD + word + bcolors.ENDC, end = ' ')\n",
        "        else:\n",
        "            print(word, end = ' ')\n",
        "    if len(text) > firstk:\n",
        "        print('...')\n",
        "    print()\n",
        "\n",
        "\n",
        "def pretty_print(results, title_query, abstract_query, firstk = 50):\n",
        "\n",
        "    title_query = clean_data(title_query)\n",
        "    abstract_query = clean_data(abstract_query)\n",
        "\n",
        "\n",
        "    for idx, res in enumerate(results):\n",
        "        # key = ID, val = [score, prof]\n",
        "\n",
        "        key, val = res\n",
        "        df = datasets[val[1]][0]\n",
        "        doc = df[df['ID'] == key].iloc[0]\n",
        "\n",
        "        print(bcolors.BOLD + f\"PaperId- {doc['ID']}\" + bcolors.ENDC)\n",
        "        print(bcolors.BOLD + f\"{idx}- Title: \" + bcolors.ENDC)\n",
        "        print_single(doc['Title'], title_query, firstk)\n",
        "        print()\n",
        "        print(bcolors.BOLD + f\"{idx}- Abstract: \" + bcolors.ENDC)\n",
        "        if len(doc['Abstract']) != 0:\n",
        "            print_single(doc['Abstract'], abstract_query, firstk)\n",
        "        else:\n",
        "            print(bcolors.FAIL + \"No abstract available\" + bcolors.ENDC)\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Caq0xjaDm1Ze"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# 0 = title, 1 = abstract\n",
        "\n",
        "def get_idf(token, type, prof):\n",
        "\n",
        "    global docs\n",
        "\n",
        "    prof_docs = docs[prof]\n",
        "\n",
        "    if token not in prof_docs:\n",
        "        return 0\n",
        "\n",
        "    sum = 0\n",
        "    for doc in prof_docs[token]:\n",
        "        if len(doc[type + 1]) > 1:\n",
        "            sum += 1\n",
        "\n",
        "    return sum\n",
        "\n",
        "def search_using_okapi(query, type, prof):\n",
        "\n",
        "    global docs\n",
        "\n",
        "    prof_docs = docs[prof]\n",
        "\n",
        "    k1 = 1.2\n",
        "    b = 0.75\n",
        "\n",
        "    query = clean_data(query)\n",
        "    N = 340\n",
        "\n",
        "    q_w = {}\n",
        "    for token in query:\n",
        "        if token not in q_w:\n",
        "            q_w[token] = 0\n",
        "        idf = get_idf(token, type, prof)\n",
        "        q_w[token] = np.log((N - idf + 0.5) / (idf + 0.5) + 1)\n",
        "\n",
        "    doc_weights = {}\n",
        "    for token in query:\n",
        "        if token not in prof_docs:\n",
        "            continue\n",
        "        for doc in prof_docs[token]:\n",
        "            if doc[0] not in doc_weights:\n",
        "                doc_weights[doc[0]] = {}\n",
        "            fq = len(doc[type + 1])\n",
        "            D = doc[type + 1][-1]\n",
        "            doc_weights[doc[0]][token] = (fq * (k1 + 1)) / (fq + k1 * (1 - b + b * D / datasets[prof][1][type]))\n",
        "\n",
        "\n",
        "    doc_scores = {}\n",
        "\n",
        "    for doc in doc_weights:\n",
        "        doc_scores[doc] = 0\n",
        "        for token in doc_weights[doc]:\n",
        "            doc_scores[doc] += q_w[token] * doc_weights[doc][token]\n",
        "\n",
        "    return doc_scores\n",
        "\n",
        "\n",
        "\n",
        "def search_query(query, type, prof, normalize = True):\n",
        "\n",
        "    global docs\n",
        "\n",
        "    prof_docs = docs[prof]\n",
        "\n",
        "    query = clean_data(query)\n",
        "    N = 340\n",
        "\n",
        "    # QUERY TF\n",
        "    q_w = {}\n",
        "    for token in query:\n",
        "        if token not in q_w:\n",
        "            q_w[token] = 1\n",
        "        else:\n",
        "            q_w[token] += 1\n",
        "\n",
        "    for token in q_w:\n",
        "        q_w[token] = 1 + np.log10(q_w[token])\n",
        "\n",
        "    # QUERY IDF\n",
        "    for token in query:\n",
        "        idf = get_idf(token, type, prof)\n",
        "        if idf == 0:\n",
        "            q_w[token] = 0\n",
        "        else:\n",
        "            q_w[token] *= np.log10(N / idf)\n",
        "\n",
        "    doc_weights = {}\n",
        "    for token in query:\n",
        "        if token not in prof_docs:\n",
        "            continue\n",
        "        for doc in prof_docs[token]:\n",
        "            if doc[0] not in doc_weights:\n",
        "                doc_weights[doc[0]] = {}\n",
        "            doc_weights[doc[0]][token] = 0 if len(doc[type+1]) == 1 else 1 + np.log10(len(doc[type + 1]) - 1)\n",
        "\n",
        "\n",
        "    doc_scores = {}\n",
        "\n",
        "    # No need to normalize query weights\n",
        "    if normalize:\n",
        "        for doc in doc_weights:\n",
        "            keys = list(doc_weights[doc].keys())\n",
        "            scores = np.array(list(doc_weights[doc].values()))\n",
        "            if np.sqrt(np.sum(scores ** 2)) != 0:\n",
        "                scores = scores / np.sqrt(np.sum(scores**2))\n",
        "            doc_weights[doc] = dict(zip(keys, scores))\n",
        "\n",
        "    for doc in doc_weights:\n",
        "        doc_scores[doc] = 0\n",
        "        for token in doc_weights[doc]:\n",
        "            doc_scores[doc] += q_w[token] * doc_weights[doc][token]\n",
        "\n",
        "    return doc_scores\n",
        "\n",
        "\n",
        "\n",
        "def search(title_query: str, abstract_query: str, max_result_count: int, method: str = 'ltn-lnn', weight: float = 0.5, pprint=False,\n",
        "           preferred_field = [1,1,1,1,1]):\n",
        "    \"\"\"\n",
        "        Finds relevant documents to query\n",
        "\n",
        "        Parameters\n",
        "        ---------------------------------------------------------------------------------------------------\n",
        "        max_result_count: Return top 'max_result_count' docs which have the highest scores.\n",
        "                          notice that if max_result_count = -1, then you have to return all docs\n",
        "\n",
        "        mode: 'detailed' for searching in title and text separately.\n",
        "              'overall' for all words, and weighted by where the word apears on.\n",
        "\n",
        "        where: when mode ='detailed', when we want search query\n",
        "                in title or text not both of them at the same time.\n",
        "\n",
        "        method: 'ltn-lnn' or 'ltc-lnc' or 'okapi25'\n",
        "\n",
        "        preferred_field: A list containing preference rate to Dr. Rabiee, Dr. Soleymani, Dr. Rohban,\n",
        "                         Dr. Kasaei, and Dr. Sharifi's papers, respectively.\n",
        "\n",
        "        Returns\n",
        "        ----------------------------------------------------------------------------------------------------\n",
        "        list\n",
        "        Retreived documents with snippet\n",
        "    \"\"\"\n",
        "\n",
        "    profs = ['Rabiee', 'Soleymani', 'Rohban', 'Kasaei', 'Sharifi']\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for idx, prof in enumerate(profs):\n",
        "        results[prof] = {}\n",
        "        if method == 'ltn-lnn':\n",
        "            results[prof]['Title'] = search_query(title_query, 0, prof, normalize=False)\n",
        "            results[prof]['Abstract'] = search_query(abstract_query, 1, prof, normalize=False)\n",
        "        elif method == 'ltc-lnc':\n",
        "            results[prof]['Title'] = search_query(title_query, 0,prof,  normalize=True)\n",
        "            results[prof]['Abstract'] = search_query(abstract_query, 1,prof, normalize=True)\n",
        "        elif method == 'okapi25':\n",
        "            results[prof]['Title'] = search_using_okapi(title_query, 0, prof)\n",
        "            results[prof]['Abstract'] = search_using_okapi(abstract_query, 1, prof)\n",
        "\n",
        "        multiplier = preferred_field[idx]\n",
        "\n",
        "        results[prof]['Title'] = \\\n",
        "         {key: value * multiplier * (1 - weight) for key, value in results[prof]['Title'].items()}\n",
        "\n",
        "        results[prof]['Abstract'] = \\\n",
        "         {key: value * multiplier * weight for key, value in results[prof]['Abstract'].items()}\n",
        "\n",
        "    cum_results = {}\n",
        "\n",
        "    for prof in profs:\n",
        "        for paper, score in results[prof]['Title'].items():\n",
        "            if paper in cum_results:\n",
        "                cum_results[paper][0] += score\n",
        "            else:\n",
        "                cum_results[paper] = [score, prof]\n",
        "        for paper, score in results[prof]['Abstract'].items():\n",
        "            if paper in cum_results:\n",
        "                cum_results[paper][0] += score\n",
        "            else:\n",
        "                cum_results[paper] = [score, prof]\n",
        "\n",
        "    result = sorted(cum_results.items(), key=lambda x: x[1][0], reverse=True)\n",
        "\n",
        "    final_result = {}\n",
        "\n",
        "    for res in result:\n",
        "        key, value = res\n",
        "        prof_name = value[1]\n",
        "        ID = datasets[prof_name][0].iloc[key - 1]['ID']\n",
        "        final_result[ID] = value\n",
        "\n",
        "    final_result = sorted(final_result.items(), key=lambda x: x[1][0], reverse=True)\n",
        "\n",
        "    if pprint == True:\n",
        "        pretty_print(final_result[:max_result_count], title_query, abstract_query)\n",
        "\n",
        "\n",
        "    return final_result[:max_result_count]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0sc1Q6Wsc4O",
        "outputId": "bc85b5cd-f57f-461e-ee16-010a789d71bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mPaperId- ec892e36c16feffdea169dbec97ecdc412778a02\u001b[0m\n",
            "\u001b[1m0- Title: \u001b[0m\n",
            "spectral and spatial complexity-based hyperspectral unmixing \n",
            "\n",
            "\u001b[1m0- Abstract: \u001b[0m\n",
            "a complexity-based bs algorithm is introduced which study the complexity of source instead of the independence and a strict theoretic interpretation is given showing that the complexity- based bs is very suitable for hyperspectral unmixing hyperspectral unmixing which decomposes pixel spectrum into a collection of constituent spectrum is a preprocessing ...\n",
            "\n",
            "\n",
            "\u001b[1mPaperId- c756b90d1d87707895aa0870730b07743957e7f0\u001b[0m\n",
            "\u001b[1m1- Title: \u001b[0m\n",
            "automatic \u001b[92m\u001b[1mdetection\u001b[0m of linguistic indicator a a mean of early prediction of alzheimer ’ s and of related dementia a cross-linguistics analysis \n",
            "\n",
            "\u001b[1m1- Abstract: \u001b[0m\n",
            "semantic scholar extracted view of `` automatic \u001b[92m\u001b[1mdetection\u001b[0m of linguistic indicator a a mean of early prediction of alzheimer ’ s and of related dementia a cross-linguistics analysis '' by vassiliki rentoumi et al \n",
            "\n",
            "\u001b[1mPaperId- 68e7f5bcb2e2c628b15a96bfa72b612bd992a8e6\u001b[0m\n",
            "\u001b[1m2- Title: \u001b[0m\n",
            "q-space novelty \u001b[92m\u001b[1mdetection\u001b[0m with variational autoencoders \n",
            "\n",
            "\u001b[1m2- Abstract: \u001b[0m\n",
            "this work proposes novelty \u001b[92m\u001b[1mdetection\u001b[0m method based on training variational autoencoders vaes on normal data to magnetic resonance imaging namely to the \u001b[92m\u001b[1mdetection\u001b[0m of diffusion-space q-space abnormality in diffusion mri scan of multiple sclerosis patient and show that many of them are able to outperform the state of the art ...\n",
            "\n",
            "\n",
            "\u001b[1mPaperId- 08f99af9f5d6d351201ee4563e407bf37bc164fd\u001b[0m\n",
            "\u001b[1m3- Title: \u001b[0m\n",
            "learning \u001b[92m\u001b[1mobject\u001b[0m motion pattern for anomaly \u001b[92m\u001b[1mdetection\u001b[0m and improved \u001b[92m\u001b[1mobject\u001b[0m \u001b[92m\u001b[1mdetection\u001b[0m \n",
            "\n",
            "\u001b[1m3- Abstract: \u001b[0m\n",
            "the proposed method provides a new higher-level layer to the traditional surveillance pipeline for anomalous event \u001b[92m\u001b[1mdetection\u001b[0m and scene model feedback and successfully used the proposed scene model to detect local a well a global anomaly in \u001b[92m\u001b[1mobject\u001b[0m track we present a novel framework for learning pattern of motion and ...\n",
            "\n",
            "\n",
            "\u001b[1mPaperId- 2f4df08d9072fc2ac181b7fced6a245315ce05c8\u001b[0m\n",
            "\u001b[1m4- Title: \u001b[0m\n",
            "rich feature hierarchy for accurate \u001b[92m\u001b[1mobject\u001b[0m \u001b[92m\u001b[1mdetection\u001b[0m and semantic segmentation \n",
            "\n",
            "\u001b[1m4- Abstract: \u001b[0m\n",
            "this paper proposes a simple and scalable \u001b[92m\u001b[1mdetection\u001b[0m algorithm that improves mean average precision map by more than 30 relative to the previous best result on voc 2012 -- achieving a map of 53.3 \u001b[92m\u001b[1mobject\u001b[0m \u001b[92m\u001b[1mdetection\u001b[0m performance a measured on the canonical pascal voc dataset ha plateaued in the last ...\n",
            "\n",
            "\n",
            "\u001b[1mPaperId- 38d4b5a464652a4afb4f043f141976f5c71c1e6b\u001b[0m\n",
            "\u001b[1m5- Title: \u001b[0m\n",
            "learning semantic scene model by \u001b[92m\u001b[1mobject\u001b[0m classification and trajectory clustering \n",
            "\n",
            "\u001b[1m5- Abstract: \u001b[0m\n",
            "in this framework the detected moving \u001b[92m\u001b[1mobject\u001b[0m are first classified a pedestrian or vehicle via a co-trained classifier which take advantage of the multiview information of \u001b[92m\u001b[1mobject\u001b[0m and can automatically learn motion pattern respectively for pedestrian and vehicle activity analysis is a basic task in video surveillance and ha become ...\n",
            "\n",
            "\n",
            "\u001b[1mPaperId- 1972c73d96353e57599962fd6059572801382212\u001b[0m\n",
            "\u001b[1m6- Title: \u001b[0m\n",
            "kernel null space method for novelty \u001b[92m\u001b[1mdetection\u001b[0m \n",
            "\n",
            "\u001b[1m6- Abstract: \u001b[0m\n",
            "this work present how to apply a null space method for novelty \u001b[92m\u001b[1mdetection\u001b[0m which map all training sample of one class to a single point which outperforms all other method for multi-class novelty \u001b[92m\u001b[1mdetection\u001b[0m detecting sample from previously unknown class is a crucial task in \u001b[92m\u001b[1mobject\u001b[0m recognition especially when dealing ...\n",
            "\n",
            "\n",
            "\u001b[1mPaperId- b9b4e05faa194e5022edd9eb9dd07e3d675c2b36\u001b[0m\n",
            "\u001b[1m7- Title: \u001b[0m\n",
            "feature pyramid network for \u001b[92m\u001b[1mobject\u001b[0m \u001b[92m\u001b[1mdetection\u001b[0m \n",
            "\n",
            "\u001b[1m7- Abstract: \u001b[0m\n",
            "this paper exploit the inherent multi-scale pyramidal hierarchy of deep convolutional network to construct feature pyramid with marginal extra cost and achieves state-of-the-art single-model result on the coco \u001b[92m\u001b[1mdetection\u001b[0m benchmark without bell and whistle feature pyramid are a basic component in recognition system for detecting \u001b[92m\u001b[1mobject\u001b[0m at different scale but ...\n",
            "\n",
            "\n",
            "\u001b[1mPaperId- 89a125d1a89bcd0c18df6810786f92d27ee4e17f\u001b[0m\n",
            "\u001b[1m8- Title: \u001b[0m\n",
            "refining the human ipsc-cardiomyocyte arrhythmic risk assessment model \n",
            "\n",
            "\u001b[1m8- Abstract: \u001b[0m\n",
            "this hcar assay showed increased performance over existing preclinical tool in predicting clinical qt prolongation arrhythmia and tdp and hips-cms are a relevant cell system to improve evaluating cardiac safety liability of drug candidate human induced pluripotent stem cell-derived cardiomyocytes hips-cms are capable of detecting drug-induced clinical arrhythmia torsade de ...\n",
            "\n",
            "\n",
            "\u001b[1mPaperId- 17d65b2f276bc3b92f4a92567becc4fe41ffcb69\u001b[0m\n",
            "\u001b[1m9- Title: \u001b[0m\n",
            "robust tracking-by-detection using a detector confidence particle filter \n",
            "\n",
            "\u001b[1m9- Abstract: \u001b[0m\n",
            "a novel approach for multi-person tracking-by-detection in a particle filtering framework that us the continuous confidence of pedestrian detector and online trained instance-specific classifier a a graded observation model which relies only on information from the past and is suitable for online application we propose a novel approach for multi-person ...\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('ec892e36c16feffdea169dbec97ecdc412778a02', [3.8509939285708876, 'Rabiee']),\n",
              " ('c756b90d1d87707895aa0870730b07743957e7f0',\n",
              "  [3.7954358191848243, 'Soleymani']),\n",
              " ('68e7f5bcb2e2c628b15a96bfa72b612bd992a8e6', [3.7318379494463954, 'Rabiee']),\n",
              " ('08f99af9f5d6d351201ee4563e407bf37bc164fd', [3.7308145831747765, 'Rabiee']),\n",
              " ('2f4df08d9072fc2ac181b7fced6a245315ce05c8',\n",
              "  [3.546124143953706, 'Soleymani']),\n",
              " ('38d4b5a464652a4afb4f043f141976f5c71c1e6b', [3.4499775772856305, 'Rabiee']),\n",
              " ('1972c73d96353e57599962fd6059572801382212', [3.4093186048120945, 'Rohban']),\n",
              " ('b9b4e05faa194e5022edd9eb9dd07e3d675c2b36',\n",
              "  [3.367176026510502, 'Soleymani']),\n",
              " ('89a125d1a89bcd0c18df6810786f92d27ee4e17f',\n",
              "  [2.9222718372083802, 'Soleymani']),\n",
              " ('17d65b2f276bc3b92f4a92567becc4fe41ffcb69', [2.8509496210761256, 'Rabiee'])]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "search(\"Object detection\", \"Object detection\", max_result_count=10, method='ltn-lnn', weight=0.5, pprint = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "fql15G4wZEX-"
      },
      "source": [
        "<div dir=\"rtl\" style=\"text-align: justify\">\n",
        "<font face=\"XB Zar\" size=4>\n",
        "    <h1>\n",
        "    <b>رتبه‌بندی نویسندگان (۲۵ نمره)</b>\n",
        "    </h1>\n",
        "</font>\n",
        "    <br>\n",
        "<font face=\"XB Zar\" size=3>  \n",
        "    برای رتبه‌بندی نویسندگان، مفهوم ارجاع نویسندگان به یکدیگر مطرح می‌شود. زمانی که نویسنده A در مقاله خود به مقاله P که نویسنده B جزو نویسندگان آن مقاله یعنی مقاله P می‌باشد، ارجاع دهد، می‌گوییم که نویسنده A به نویسنده B ارجاع داده است. با توجه به این رابطه، می‌توان گراف ارجاعات بین نویسندگان را ایجاد و سپس با استفاده از الگوریتم HITS\n",
        "نویسندگان را رتبه‌بندی کرد. برای رتبه‌بندی نیاز است تا از شاخص‌های hub و authority استفاده کنیم.\n",
        "\n",
        "\n",
        "</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cCo9-ARZEX_",
        "outputId": "9e39f8cc-f7f9-43e7-c974-cb1f9b391804"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Piero Carninci 0.14031809520955943\n",
            "Thomas R. Gingeras 0.11265581495400442\n",
            "Steven E. Brenner 0.1113531289314367\n",
            "Takeya Kasukawa 0.11046166735060929\n",
            "Michael Richard Brent 0.09898872214781862\n",
            "Hidemasa Bono 0.0988578807111415\n",
            "Bing Ren 0.0981267386333651\n",
            "Pouya Kheradpour 0.0973009927557432\n",
            "Manolis Kellis 0.0973009927557432\n",
            "Nicolas N. Negre 0.09265530915807109\n"
          ]
        }
      ],
      "source": [
        "import networkx as nx\n",
        "import networkx as nx\n",
        "\n",
        "\n",
        "def hit_algorithm(n):\n",
        "    \"\"\"\n",
        "        Implementing the HITS algorithm to score authors based on their papers and co-authors.\n",
        "\n",
        "        Parameters\n",
        "        ---------------------------------------------------------------------------------------------------\n",
        "        papers: A list of paper dictionaries with the following keys:\n",
        "                \"id\": A unique ID for the paper\n",
        "                \"title\": The title of the paper\n",
        "                \"abstract\": The abstract of the paper\n",
        "                \"date\": The year in which the paper was published\n",
        "                \"authors\": A list of the names of the authors of the paper\n",
        "                \"related_topics\": A list of IDs for related topics (optional)\n",
        "                \"citation_count\": The number of times the paper has been cited (optional)\n",
        "                \"reference_count\": The number of references in the paper (optional)\n",
        "                \"references\": A list of IDs for papers that are cited in the paper (optional)\n",
        "        n: An integer representing the number of top authors to return.\n",
        "\n",
        "        Returns\n",
        "        ---------------------------------------------------------------------------------------------------\n",
        "        List\n",
        "        list of the top n authors based on their hub scores.\n",
        "    \"\"\"\n",
        "    # Create a graph of authors and papers (all of the authors and papers represented as nodes, and all of the authors who wrote each paper connected to the corresponding paper node by an edge)\n",
        "    G = {}\n",
        "\n",
        "\n",
        "    for prof in profs:\n",
        "        for paper in crawled[prof]:\n",
        "            for author in paper['Authors']:\n",
        "                if author in G and paper['ID'] not in G[author]:\n",
        "                    G[author].append(paper['ID'])\n",
        "                else:\n",
        "                    G[author] = [paper['ID']]\n",
        "\n",
        "    nodes = set(G.keys()).union(*G.values())\n",
        "\n",
        "    adjacency_matrix = {node: [0] * len(nodes) for node in nodes}\n",
        "\n",
        "    for node, outgoing_edges in G.items():\n",
        "        for edge in outgoing_edges:\n",
        "            adjacency_matrix[node][list(nodes).index(edge)] = 1\n",
        "\n",
        "    matrix = np.array(list(adjacency_matrix.values()))\n",
        "\n",
        "    (N, _) = matrix.shape\n",
        "\n",
        "    hubs = np.ones(N)\n",
        "    authorities = np.ones(N)\n",
        "\n",
        "\n",
        "    for i in range(5):\n",
        "        hubs = matrix @ authorities\n",
        "        authorities = matrix.T @ hubs\n",
        "\n",
        "        hubs = hubs / np.linalg.norm(hubs)\n",
        "        authorities = authorities / np.linalg.norm(authorities)\n",
        "\n",
        "\n",
        "    # Sort the lists based on the values of hubs\n",
        "    sorted_lists = sorted(zip(hubs, nodes), reverse = True)[:n]\n",
        "    sorted_hubs, sorted_nodes = zip(*sorted_lists)\n",
        "\n",
        "    return sorted_nodes, sorted_hubs\n",
        "\n",
        "\n",
        "# call the hit_algorithm function\n",
        "top_authors, scores = hit_algorithm(10)\n",
        "\n",
        "for (author, score) in zip(top_authors, scores):\n",
        "    print(author, score)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "dcrlBXAfZEX_"
      },
      "source": [
        "<div dir=\"rtl\">\n",
        "<font face=\"XB Zar\" size=4>\n",
        "    <h1>\n",
        "    <b>سیستم پیشنهادگر (۲۰ نمره)</b>\n",
        "    </h1>\n",
        "</font>\n",
        "<font face=\"XB Zar\" size=3>\n",
        "\n",
        "در این بخش سعی می‌کنیم که یک سیستم پیشنهادگر مقالات بر اساس جست‌و‌جو‌ها یا علايق یک کاربر پیاده‌سازی کنیم، سیستم پیشنهاد دهنده‌ای که قصد داریم آن را ایجاد کنیم،‌ باید بتواند بر اساس لیستی از مقالاتی که کاربر قبلا آن‌ها را مطالعه کرده یا به آن‌ها علاقه داشته است، مقالات تازه انتشار یافته‌‌ی جدید را به کاربر پیشنهاد دهد.\n",
        "\n",
        "در فایل recommended_papers.json\n",
        "لیستی از کاربران قرار دارد که در فیلد positive_papers هر کاربر،\n",
        "تعداد ۵۰ مقاله از مقالاتی که کاربر به آن‌ها علاقه داشته است مشخص شده است. و همچینین در فیلد recommendedPapers هر کاربر تعداد ۱۰ مقاله به ترتیب اهمیت، از مقالات جدیدی که کاربر آن‌ها را پسندیده است قرار دارد.\n",
        "\n",
        "در این بخش هدف شما یادگیری سیستم پیشنهاد‌ دهنده بر اساس همین داده‌ها می‌باشد، و به عبارتی شما بایستی کاربر‌ها را به دو دسته آموزش و آزمایش تقسیم کنید، و بر اساس داده‌های آموزشی بتوانید مقالات جدید مورد پسند کاربرهای آزمایش را پیش‌بینی کنید. (بنابراین در این پیش‌بینی نمی‌توانید از فیلد recommendedPapers این کاربران استفاده کنید.)\n",
        "\n",
        "</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "vihEsGa6ZEX_"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open('/content/drive/MyDrive/MIR/Assignment 3/recommended_papers.json', 'r') as fp:\n",
        "    recommended_papers = json.load(fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "PnFRcRlkZEYA"
      },
      "outputs": [],
      "source": [
        "sample_user = recommended_papers[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwRLTa5oZEYA",
        "outputId": "b4d61a52-7aa6-48d4-f3a0-1738922fd4f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "d9404b4a794c07b5e2cdf3203aabf06d70c6be9b\n",
            "CENTAURO: A Hybrid Locomotion and High Power Resilient Manipulation Platform\n",
            "Despite the development of a large number of mobile manipulation robots, very few platforms can demonstrate the required strength and mechanical sturdiness to accommodate the needs of real-world applications with high payload and moderate/harsh physical interaction demands, e.g., in disaster-response scenarios or heavy logistics/collaborative tasks. In this letter, we introduce the design of a wheeled-legged mobile manipulation platform capable of executing demanding manipulation tasks, and demonstrating significant physical resilience while possessing a body size (height/width) and weight compatible to that of a human. The achieved performance is the result of combining a number of design and implementation principles related to the actuation system, the integration of body structure and actuation, and the wheeled-legged mobility concept. These design principles are discussed, and the solutions adopted for various robot components are detailed. Finally, the robot performance is demonstrated in a set of experiments validating its power and strength capability when manipulating heavy payload and executing tasks involving high impact physical interactions.\n",
            "['Computer Science']\n"
          ]
        }
      ],
      "source": [
        "print(sample_user['positive_papers'][0]['paperId'])\n",
        "print(sample_user['positive_papers'][0]['title'])\n",
        "print(sample_user['positive_papers'][0]['abstract'])\n",
        "print(sample_user['positive_papers'][0]['fieldsOfStudy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXl9UlMiZEYB",
        "outputId": "fe6958d9-196b-45d3-c482-0baad3745436"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "94eebbefe8a37cf394be899b85af295c2e3a1f01\n",
            "Efficient Parametric Approximations of Neural Network Function Space Distance\n",
            "It is often useful to compactly summarize important properties of model parameters and training data so that they can be used later without storing and/or iterating over the entire dataset. As a specific case, we consider estimating the Function Space Distance (FSD) over a training set, i.e. the average discrepancy between the outputs of two neural networks. We propose a Linearized Activation Function TRick (LAFTR) and derive an efficient approximation to FSD for ReLU neural networks. The key idea is to approximate the architecture as a linear network with stochastic gating. Despite requiring only one parameter per unit of the network, our approach outcompetes other parametric approximations with larger memory requirements. Applied to continual learning, our parametric approximation is competitive with state-of-the-art nonparametric approximations, which require storing many training examples. Furthermore, we show its efficacy in estimating influence functions accurately and detecting mislabeled examples without expensive iterations over the entire dataset.\n",
            "['Computer Science', 'Mathematics']\n"
          ]
        }
      ],
      "source": [
        "print(sample_user['recommendedPapers'][0]['paperId'])\n",
        "print(sample_user['recommendedPapers'][0]['title'])\n",
        "print(sample_user['recommendedPapers'][0]['abstract'])\n",
        "print(sample_user['recommendedPapers'][0]['fieldsOfStudy'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(recommended_papers,test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "vhmoSa51xNOx"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fields = set()\n",
        "papers = set()\n",
        "\n",
        "corpus = []\n",
        "\n",
        "for user in train:\n",
        "    for ppaper in user['positive_papers']:\n",
        "        if ppaper['paperId'] not in papers:\n",
        "            papers.add(ppaper['paperId'])\n",
        "            corpus.append(ppaper['title'].lower())\n",
        "            corpus.append(ppaper['abstract'].lower())\n",
        "        if ppaper['fieldsOfStudy'] is not None:\n",
        "            fields.update(ppaper['fieldsOfStudy'])\n",
        "\n",
        "fields = list(fields)\n",
        "papers = list(papers)"
      ],
      "metadata": {
        "id": "S0_e6KRbzVH0"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "N, M = len(train), len(fields)\n",
        "\n",
        "user_embeddings = np.zeros((N,M))\n",
        "\n",
        "for idx, user in enumerate(train):\n",
        "    for ppaper in user['positive_papers']:\n",
        "        if ppaper['fieldsOfStudy'] is not None:\n",
        "            for field in ppaper['fieldsOfStudy']:\n",
        "                 f_idx = fields.index(field)\n",
        "                 user_embeddings[idx, f_idx] += 1\n",
        "\n",
        "    user_embeddings[idx] = user_embeddings[idx] / len(user['positive_papers'])"
      ],
      "metadata": {
        "id": "PxUMTmJM0eHH"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "_7NrU7XVZEYB"
      },
      "source": [
        "<div dir=\"rtl\">\n",
        "<font face=\"XB Zar\" size=4>\n",
        "    <h2>\n",
        "    <b>روش Collaborative Filtering (۱۰ نمره)</b>\n",
        "    </h2>\n",
        "</font>\n",
        "<font face=\"XB Zar\" size=3>\n",
        "\n",
        "در این راهکار سعی می‌کنیم با استفاده از کاربران مشابه با یک کاربر، سلیقه‌ی او را حدس بزنیم و مقالاتی را که کاربران مشابه دیده‌اند را به کاربر نمایش دهیم.\n",
        "\n",
        "در این روش ابتدا باید $N$ کاربر که سلیقه‌ی مشابه با کاربر $x$ دارند را پیدا کنید، و با ترکیب لیست مقالات جدید مورد علاقه‌ی آن $N$ کاربر مشابه،\n",
        " ۱۰ مقاله‌ به کاربر $x$ پیشنهاد دهید.\n",
        "\n",
        "توجه داشته باشید که برای اینکه شباهت دو کاربر را پیدا کنید، باید cosine_similarity بین بردار زمینه‌های مورد علاقه‌ی دو کاربر استفاده کنید. این بردار از $M$ درایه تشکیل شده است، که $M$ تعداد زمینه‌های یکتاییست که در داده‌ها وجود دارد. و در این بردار درایه‌ی $j$ام\n",
        "نشان دهنده‌ی نسبت تعداد مقالات خوانده‌ی شده‌ کاربر در زمینه‌ی $j$ به تعداد کل مقاله‌های خوانده شده توسط او می‌باشد. (توجه کنید که هر مقاله می‌تواند چند زمینه داشته باشد و بنابراین حاصل جمع درایه‌های این بردار الزاما یک نمی‌باشد)\n",
        "\n",
        "</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "3BduHWtFZEYC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78e6d043-ccc2-4070-c0ce-57865faa94f9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['2714c11c0809d638e1e501831913671914407e5d',\n",
              " 'ba852c774c00894376bc20cdccb884b0dbe1196b',\n",
              " '590b8f5e17424d1d4be560a0d2b1c665d8d3c7f8',\n",
              " '66fa8acfbf6dc1022d5aa2ee43fad20cda231f98',\n",
              " '5cbadc7545b5296a8b245be20c78f8b9b628973c',\n",
              " '3256e193a308e451c7107e51bb96c3e9b5bb6ae3',\n",
              " 'b65dfaf9b95b21840848b3b77bb4df655305ac89',\n",
              " '908bdaa3588ac073d06b4452ffd5fce7bd9af042',\n",
              " '33320785835ba8cdd717cb8d043e99e942e0b491',\n",
              " '600e824cadf41e3543a4c0db22226aeb1578bce9']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "import heapq\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "def get_embedding(user):\n",
        "    embed = np.zeros(len(fields))\n",
        "    for ppaper in user['positive_papers']:\n",
        "        if ppaper['fieldsOfStudy'] is not None:\n",
        "            for field in ppaper['fieldsOfStudy']:\n",
        "                 f_idx = fields.index(field)\n",
        "                 embed[f_idx] += 1\n",
        "\n",
        "    return embed / len(user['positive_papers'])\n",
        "\n",
        "\n",
        "def collaborative_filtering(user_id: int, N=10):\n",
        "    \"\"\"\n",
        "    Returns the top 10 related articles to the user, based on similar users (Similar users should be on \"train data\").\n",
        "\n",
        "    Parameters:\n",
        "    user_id (int): The unique index of the user.\n",
        "    N: The number of hyperparameter N in Nearest Neighbor algorithm.\n",
        "\n",
        "    Returns:\n",
        "    List[str]: A list of 10 article IDs that are most relevant to the user's interests.\n",
        "    \"\"\"\n",
        "\n",
        "    user = test[user_id]\n",
        "    user_embed = get_embedding(user)\n",
        "\n",
        "    similarity = cosine_similarity(user_embeddings, user_embed.reshape(1, -1)).reshape(len(user_embeddings))\n",
        "\n",
        "    sorted_indices = np.argsort(similarity)[::-1][:N]\n",
        "\n",
        "    scores = similarity[sorted_indices]\n",
        "\n",
        "    potential_papers = {}\n",
        "\n",
        "    for idx in sorted_indices:\n",
        "        user = train[idx]\n",
        "        for paper in user['recommendedPapers']:\n",
        "            if paper['paperId'] in potential_papers:\n",
        "                potential_papers[paper['paperId']] += 1\n",
        "            else:\n",
        "                potential_papers[paper['paperId']] = 1\n",
        "\n",
        "\n",
        "    return heapq.nlargest(10, potential_papers, key=potential_papers.get)\n",
        "\n",
        "\n",
        "collaborative_filtering(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "kd4DLwrEZEYC"
      },
      "source": [
        "<div dir=\"rtl\">\n",
        "<font face=\"XB Zar\" size=4>\n",
        "    <h2>\n",
        "    <b>روش Content Based (۱۰ نمره)</b>\n",
        "    </h2>\n",
        "</font>\n",
        "<font face=\"XB Zar\" size=3>\n",
        "\n",
        "در این روش با استفاده از مقالات قبلی که کاربر آن‌ها را پسندیده است، به کاربر مقاله‌ی جدید پیشنهاد می‌دهیم.\n",
        "\n",
        "برای اینکار ابتدا تمام مقالات پیشنهاد شده برای تمام کاربرها را سر جمع کنید. (در واقع مدلی که پیاده‌سازی می‌کنید نباید بداند که به کدام کاربر چه مقالاتی پیشنهاد شده است)\n",
        "\n",
        "سپس بردار tf-idf برای تایتل هر یک از مقالات را ایجاد کنید، و میانگین بردار مقالات مورد علاقه‌ی هر فرد را با لیستی که از مقالات جدید سر جمع کردید مقایسه کنید و ۱۰ تا از شبیه‌ترین مقالات را خروجی دهید.\n",
        "\n",
        "</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "vectorizer.fit(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "_ILRUVLdFMWh",
        "outputId": "655903cd-6b1c-45a7-9cc5-fdd26d61ba94"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seen = OrderedSet()\n",
        "\n",
        "content_embeddings = []\n",
        "\n",
        "for user in train:\n",
        "    for paper in user['recommendedPapers']:\n",
        "        if paper['paperId'] in seen:\n",
        "            continue\n",
        "        else:\n",
        "            seen.add(paper['paperId'])\n",
        "            content_embeddings.append(paper['title'].lower())\n",
        "content_embeddings = vectorizer.transform(content_embeddings)"
      ],
      "metadata": {
        "id": "1EF1W0Lm6-pY"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "5QPA3vqUZEYC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe560dc8-da2e-48cd-ad8a-abc3a953d314"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['75c9a135b1e870881c8a5e3aff18a7e5914cee89',\n",
              "       '4ef86160b00fbcd0e378f07358fae1296fee081a',\n",
              "       '377e5e0e5f4253442ad1e476c076a3928b031811',\n",
              "       '6625a66e96821efed9b19e81d86bda7d66931020',\n",
              "       '45a1f5d37bc078a2d6c4354f7aba46e8e2228ae8',\n",
              "       '8555c806481355c18d195b451113614d6f4ef606',\n",
              "       '7ead40d4cdcf83b2c53af30dda2a2dfbf4bb88a3',\n",
              "       'efeaa77602c13e521e8359d2dad21228cac435f6',\n",
              "       '77cbd12733f273c9ec8c0cd524c02e87d811b477',\n",
              "       'c92eed4bfe1f55ada9e488f26cb3c4c9231e6b27'], dtype='<U40')"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "def content_based_recommendation(user_id):\n",
        "    \"\"\"\n",
        "    Returns the top 10 related articles to the user, based on the titles of the articles.\n",
        "\n",
        "    Parameters:\n",
        "    user_id (int): The unique index of the user.\n",
        "\n",
        "    Returns:\n",
        "    List[str]: A list of 10 article IDs that are most relevant to the user's interests.\n",
        "    \"\"\"\n",
        "\n",
        "    global content_embeddings, seen\n",
        "\n",
        "    user_papers = []\n",
        "\n",
        "    for paper in test[user_id]['positive_papers']:\n",
        "        user_papers.append(paper['title'])\n",
        "\n",
        "    user_embed = np.sum(vectorizer.transform(user_papers), axis = 0) / len(user_papers)\n",
        "\n",
        "    recom_embeddings = content_embeddings.toarray()\n",
        "\n",
        "\n",
        "    similarity = cosine_similarity(recom_embeddings, np.array(user_embed)).reshape(len(recom_embeddings))\n",
        "\n",
        "\n",
        "    sorted_indices = np.argsort(similarity)[::-1][:10]\n",
        "\n",
        "\n",
        "    return np.array(seen)[sorted_indices]\n",
        "\n",
        "content_based_recommendation(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "APb-UiFuZEYC"
      },
      "source": [
        "<div dir=\"rtl\">\n",
        "<font face=\"XB Zar\" size=4>\n",
        "    <h2>\n",
        "    <b>ارزیابی سیستم‌های پیشنهادگر</b>\n",
        "    </h2>\n",
        "</font>\n",
        "<font face=\"XB Zar\" size=3>\n",
        "\n",
        "در این بخش سیستم‌های پیشنهادگری را که پیاده کرده‌اید را با استفاده از معیار nDCG و با استفاده از دادگان واقعی از علایق کاربران نسبت به مقالات جدید ارزیابی کنید و نتایج حاصل از دو روش را با هم مقایسه کنید.\n",
        "\n",
        "</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Vu279F9-ZEYD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "761a0bfa-8487-4164-bbb0-05b6642280a1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5071940845871584"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "\"\"\"\n",
        "TODO: Compare two methods with nDCG metric.\n",
        "\"\"\"\n",
        "\n",
        "def ndcg(pred, real):\n",
        "    Z = 0\n",
        "    DCG = 0\n",
        "    for i in range(1, 11):\n",
        "        Z += (2 ** (11 - i) - 1) / np.log(i + 1)\n",
        "        if pred[i - 1] not in real:\n",
        "            continue\n",
        "        else:\n",
        "            idx = 10 - real.index(pred[i - 1])\n",
        "            DCG += (2 ** (idx) - 1) / np.log(i + 1)\n",
        "\n",
        "    return DCG / Z\n",
        "\n",
        "def measure_colab():\n",
        "    average = 0\n",
        "    for i in range(len(test)):\n",
        "        pred = collaborative_filtering(i)\n",
        "        real = []\n",
        "        for p in test[i]['recommendedPapers']:\n",
        "            real.append(p['paperId'])\n",
        "        average += ndcg(pred, real)\n",
        "    return average / len(test)\n",
        "\n",
        "\n",
        "measure_colab()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def measure_content():\n",
        "    average = 0\n",
        "    for i in range(len(test)):\n",
        "        pred = content_based_recommendation(i)\n",
        "        real = []\n",
        "        for p in test[i]['recommendedPapers']:\n",
        "            real.append(p['paperId'])\n",
        "        average += ndcg(pred, real)\n",
        "    return average / len(test)\n",
        "\n",
        "measure_content()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZZw3dzP-mcE",
        "outputId": "026cbf40-782f-4702-b792-cde358cf78e6"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.053164150508337706"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collaborative Filtering tends to have higher accuracy due to its ability to capture complex user behavior and make personalized recommendations. It can uncover hidden patterns and offer serendipitous suggestions, even for users with sparse data. Content-based methods, while effective in recommending similar items, may struggle with the \"cold start\" problem and lack diversity in recommendations. Collaborative Filtering's reliance on user behavior data enables it to provide more accurate and diverse recommendations, leading to higher accuracy."
      ],
      "metadata": {
        "id": "u_JYXA45AVnL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "n_E70KI8ZEYD"
      },
      "source": [
        "<div dir=\"rtl\" style=\"text-align: justify\">\n",
        "<font face=\"XB Zar\" size=4>\n",
        "    <h1>\n",
        "    <b>رابط کاربری (تا ۱۰ نمره)</b>\n",
        "    </h1>\n",
        "</font>\n",
        "    <br>\n",
        "<font face=\"XB Zar\" size=3>\n",
        "در این بخش\n",
        " باید یک واسط کاربری ساده برای اجرای تعاملی بخش‌های مختلف سیستم که از فاز ۱ ساخته‌اید و همچنین مشاهده نتایج پیاده‌سازی کنید. در صورت پیاده سازی زیبا و بهتر رابط کاربری تا ده نمره نمره امتیازی نیز در نظر گرفته خواهد شد.\n",
        "</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3Lwtf_HZEYD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "08ac30a6a1fd2e576b33e03f7d61c3a285d7ee0582c2dd23dde6343ef303ebe9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}